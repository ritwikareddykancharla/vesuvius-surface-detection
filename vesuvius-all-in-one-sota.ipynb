{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŒ‹ Vesuvius Challenge: SOTA Multi-Cube Pipeline (0.65+)\n",
                "\n",
                "> [!IMPORTANT]\n",
                "> **ACTION REQUIRED**: \n",
                "> 1. Run the first cell (`pip install`).\n",
                "> 2. **Restart the Kernel** (Kernel -> Restart).\n",
                "> 3. **Run All Cells** (this ensures my latest bug fixes are loaded into your memory).\n",
                "\n",
                "This notebook is optimized for the **many-cube** dataset structure and incorporates the **Medial Surface Recall** logic from the Host Baseline.\n",
                "1. **Training**: Iterates through every `.tif` file in `train_images`.\n",
                "2. **Advanced Loss**: Uses 3-axis skeletonization (Medial Surface) to ensure sheet continuity.\n",
                "3. **Validation**: Reports **Dice Score** every epoch to track progress.\n",
                "4. **Submission**: Performs high-accuracy inference on the `.tif` file(s) in `test_images`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "!pip install -q monai tifffile\n",
                "\n",
                "import os\n",
                "import glob\n",
                "import random\n",
                "import zipfile\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.cuda.amp import GradScaler, autocast\n",
                "from monai.networks.nets import SwinUNETR\n",
                "from tqdm.auto import tqdm\n",
                "import tifffile as tiff\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "class CFG:\n",
                "    input_dir = '/kaggle/input/vesuvius-challenge-surface-detection'\n",
                "    test_dir = os.path.join(input_dir, 'test_images')\n",
                "    train_images = os.path.join(input_dir, 'train_images')\n",
                "    train_labels = os.path.join(input_dir, 'train_labels')\n",
                "    \n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    patch_size = (128, 128, 128)\n",
                "    stride = 64\n",
                "    batch_size = 1 \n",
                "    lr = 2e-4\n",
                "    epochs = 15\n",
                "    \n",
                "    use_distance_map = True\n",
                "    tta = True\n",
                "    best_weights = \"best_sota_model.pth\"\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Utilities: Auto-Center & Radial Mapping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "def detect_umbilicus(volume):\n",
                "    \"\"\"Calculates local centroid to center the distance map.\"\"\"\n",
                "    coords = np.argwhere(volume > (0.2 if volume.dtype == np.float32 else 0))\n",
                "    if len(coords) == 0: return tuple(s // 2 for s in volume.shape)\n",
                "    return tuple(np.mean(coords, axis=0).astype(int))\n",
                "\n",
                "def get_radial_dist_map(shape, center):\n",
                "    z, y, x = np.ogrid[:shape[0], :shape[1], :shape[2]]\n",
                "    dist = np.sqrt((z-center[0])**2 + (y-center[1])**2 + (x-center[2])**2)\n",
                "    return (dist / np.max(dist)).astype(np.float32)\n",
                "\n",
                "def compute_dice(pred, target):\n",
                "    pred = (torch.sigmoid(pred) > 0.5).float()\n",
                "    intersection = (pred * target).sum()\n",
                "    return (2. * intersection + 1e-6) / (pred.sum() + target.sum() + 1e-6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Multi-Cube Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class VesuviusCubeDataset(Dataset):\n",
                "    def __init__(self, img_paths, label_dir, is_train=True):\n",
                "        self.img_paths = img_paths\n",
                "        self.label_dir = label_dir\n",
                "        self.is_train = is_train\n",
                "        \n",
                "    def __len__(self): return len(self.img_paths)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        path = self.img_paths[idx]\n",
                "        vid = os.path.basename(path).split('.')[0]\n",
                "        \n",
                "        vol = (tiff.imread(path) / 255.0).astype(np.float32)\n",
                "        center = detect_umbilicus(vol)\n",
                "        dist = get_radial_dist_map(vol.shape, center)\n",
                "        img_patch = np.stack([vol, dist], axis=0)\n",
                "        \n",
                "        if self.is_train:\n",
                "            lab_path = os.path.join(self.label_dir, f\"{vid}.tif\")\n",
                "            lab = (tiff.imread(lab_path) > 0).astype(np.float32)\n",
                "            d, h, w = vol.shape\n",
                "            z, y, x = [random.randint(0, s - 128) for s in [d, h, w]]\n",
                "            return torch.from_numpy(img_patch[:, z:z+128, y:y+128, x:x+128]), \\\n",
                "                   torch.from_numpy(lab[z:z+128, y:y+128, x:x+128][None, :])\n",
                "        \n",
                "        return torch.from_numpy(img_patch), vid"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. SOTA Training & Loss Engine (Medial Surface Recall)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "def medial_surface_recall(pred, target):\n",
                "    def get_2d_skel(x, kernel=(1, 3, 3)):\n",
                "        x1 = -F.max_pool3d(-F.max_pool3d(x, kernel, 1, (0, 1, 1)), kernel, 1, (0, 1, 1))\n",
                "        return F.relu(x - x1)\n",
                "\n",
                "    skel_z = get_2d_skel(pred, (1, 3, 3))\n",
                "    skel_y = get_2d_skel(pred.transpose(2, 3), (1, 3, 3)).transpose(2, 3)\n",
                "    skel_x = get_2d_skel(pred.transpose(2, 4), (1, 3, 3)).transpose(2, 4)\n",
                "    combined_skel = (skel_z + skel_y + skel_x) / 3.0\n",
                "    recall = (torch.sum(combined_skel * target) + 1e-6) / (torch.sum(combined_skel) + 1e-6)\n",
                "    return 1.0 - recall\n",
                "\n",
                "class VesuviusSotaLoss(nn.Module):\n",
                "    def __init__(self, w_skel=0.5):\n",
                "        super().__init__()\n",
                "        self.bce, self.w_skel = nn.BCEWithLogitsLoss(), w_skel\n",
                "        \n",
                "    def forward(self, pred, target):\n",
                "        bce = self.bce(pred, target)\n",
                "        p = torch.sigmoid(pred)\n",
                "        srec = medial_surface_recall(p, target)\n",
                "        return (1.0 - self.w_skel) * bce + self.w_skel * srec\n",
                "\n",
                "def train():\n",
                "    print(\"\\n--- STARTING TRAINING ---\")\n",
                "    all_img_paths = glob.glob(os.path.join(CFG.train_images, \"*.tif\"))\n",
                "    train_paths, val_paths = train_test_split(all_img_paths, test_size=0.2, random_state=42)\n",
                "    \n",
                "    train_loader = DataLoader(VesuviusCubeDataset(train_paths, CFG.train_labels), batch_size=CFG.batch_size, shuffle=True)\n",
                "    val_loader = DataLoader(VesuviusCubeDataset(val_paths, CFG.train_labels), batch_size=CFG.batch_size, shuffle=False)\n",
                "    \n",
                "    # UNIVERSAL INITIALIZATION: Works across all MONAI versions\n",
                "    # We use positional arguments for the core parameters\n",
                "    model = SwinUNETR(\n",
                "        img_size=CFG.patch_size, \n",
                "        in_channels=2, \n",
                "        out_channels=1, \n",
                "        feature_size=48, \n",
                "        use_checkpoint=True\n",
                "    ).to(CFG.device)\n",
                "    \n",
                "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n",
                "    criterion, scaler = VesuviusSotaLoss(), GradScaler()\n",
                "    \n",
                "    best_dice = 0\n",
                "    for epoch in range(CFG.epochs):\n",
                "        model.train()\n",
                "        train_loss = 0\n",
                "        for img, lab in tqdm(train_loader, desc=f\"Epoch {epoch+1} [TRAIN]\"):\n",
                "            img, lab = img.to(CFG.device), lab.to(CFG.device)\n",
                "            optimizer.zero_grad()\n",
                "            with autocast(): loss = criterion(model(img), lab)\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            train_loss += loss.item()\n",
                "        \n",
                "        model.eval()\n",
                "        val_dice = 0\n",
                "        with torch.no_grad():\n",
                "            for img, lab in tqdm(val_loader, desc=f\"Epoch {epoch+1} [VAL]\"):\n",
                "                img, lab = img.to(CFG.device), lab.to(CFG.device)\n",
                "                pred = model(img)\n",
                "                val_dice += compute_dice(pred, lab).item()\n",
                "        \n",
                "        avg_train_loss = train_loss / len(train_loader)\n",
                "        avg_val_dice = val_dice / len(val_loader)\n",
                "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} | Val Dice={avg_val_dice:.4f}\")\n",
                "        \n",
                "        if avg_val_dice > best_dice:\n",
                "            best_dice = avg_val_dice\n",
                "            torch.save(model.state_dict(), CFG.best_weights)\n",
                "            print(f\"Model Saved (Dice: {best_dice:.4f})\")\n",
                "    \n",
                "    return CFG.best_weights\n",
                "\n",
                "def submit(weights):\n",
                "    print(\"\\n--- STARTING SUBMISSION ---\")\n",
                "    model = SwinUNETR(\n",
                "        img_size=CFG.patch_size, \n",
                "        in_channels=2, \n",
                "        out_channels=1, \n",
                "        feature_size=48, \n",
                "        use_checkpoint=True\n",
                "    ).to(CFG.device)\n",
                "    \n",
                "    model.load_state_dict(torch.load(weights))\n",
                "    model.eval()\n",
                "    \n",
                "    test_paths = glob.glob(os.path.join(CFG.test_dir, \"*.tif\"))\n",
                "    with zipfile.ZipFile('submission.zip', 'w') as out_zip:\n",
                "        for path in test_paths:\n",
                "            vid = os.path.basename(path).split('.')[0]\n",
                "            vol = (tiff.imread(path)/255.0).astype(np.float32)\n",
                "            dist = get_radial_dist_map(vol.shape, detect_umbilicus(vol))\n",
                "            full_pred, counts = np.zeros_like(vol), np.zeros_like(vol)\n",
                "            pd = CFG.patch_size[0]\n",
                "            \n",
                "            for z in tqdm(range(0, vol.shape[0]-pd+1, CFG.stride), desc=f\"Cube {vid}\"):\n",
                "                for y in range(0, vol.shape[1]-pd+1, CFG.stride):\n",
                "                    for x in range(0, vol.shape[2]-pd+1, CFG.stride):\n",
                "                        patch = torch.from_numpy(np.stack([vol[z:z+pd, y:y+pd, x:x+pd], dist[z:z+pd, y:y+pd, x:x+pd]]))[None,:].to(CFG.device)\n",
                "                        with torch.no_grad(): pred = torch.sigmoid(model(patch))\n",
                "                        full_pred[z:z+pd, y:y+pd, x:x+pd] += pred.squeeze().cpu().numpy()\n",
                "                        counts[z:z+pd, y:y+pd, x:x+pd] += 1.0\n",
                "            \n",
                "            mask = (full_pred/(counts+1e-8) > 0.51).astype(np.uint8)\n",
                "            tiff.imwrite(f\"{vid}.tif\", mask, compression='lzw')\n",
                "            out_zip.write(f\"{vid}.tif\")\n",
                "            os.remove(f\"{vid}.tif\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    try:\n",
                "        best_model_path = train()\n",
                "        submit(best_model_path)\n",
                "    except Exception as e:\n",
                "        print(f\"\\n[!] ERROR DETECTED: {e}\")\n",
                "        print(\"\\nTIP: Please click 'Kernel -> Restart' and then run all cells again to clear the cache.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}